stopwords("portuguese")
library(tm)
library(wordcloud)
library(RColorBrewer)
stopwords("portuguese")
install.packages("tm")
install.packages("textreadr")
install.packages("wordcloud")
library(tm)
library(textreadr)
library(wordcloud)
library(RColorBrewer)
# Fontes disponíveis
getSources()
# Formatos
getReaders()
# Criar corpus -> PCorpus(): físico; VCorpus(): volátil em memória
# Pegamos dados da Wikipedia
arquivo_cru = read_pdf("C:/Users/Usuario/Documents/Mestrado/FT095/Artigo/Recursos Humanos em saude_do processo intuitivo ao People Analytics.pdf", skip = 16)
arquivo_tratado = arquivo_cru[ ,3]
# Geramos um corpus
corpus_final = VCorpus(VectorSource(arquivo_tratado), readerControl = list(reader = readPlain, language = "eng"))
# tm_map para executar transformações
stopwords("portuguese")
# Remove
palavras = c("the", "ser", "silva", "nursing", "martinez", "martinez ", "francisco", "silva ", " silva", "Silva", "Martinez", "Francisco")
corpus_final = tm_map(corpus_final, removeWords, c(stopwords("portuguese"), palavras))
# Excesso de espaços em branco
corpus_final = tm_map(corpus_final, stripWhitespace)
# Pontuação
corpus_final = tm_map(corpus_final, removePunctuation)
# Números
corpus_final = tm_map(corpus_final, removeNumbers)
wordcloud(corpus_final, max.words = 20, random.order = T, colors = rainbow(8), rot.per = 0.5, use.r.layout = T)
# Matriz de palavras frequentes
# TermDocumentMatrix -> termos na linha
# DocumentMatrix -> documentos na linha
freq = TermDocumentMatrix(corpus_final)
freq
# Transformando em matrix do R
matriz = as.matrix(freq)
# Ordeno de acordo com a frequência
matriz = sort(rowSums(matriz), decreasing = TRUE)
# Data frame
matriz = data.frame(word = names(matriz), freq = matriz)
head(matriz, n = 100)
# Encontra termos mais frequentes
findFreqTerms(freq, 20, Inf)
removeSparseTerms(freq, 0.4)
wordcloud(matriz$word, matriz$freq, max.words = 20, random.order = T, colors = rainbow(8), rot.per = 0.5, use.r.layout = T)
arquivo_cru
inspect(corpus_final)
as.character(corpus_final)
# Encontrando os 20 termos mais frequentes
findFreqTerms(freq, 20, Inf)
removeSparseTerms(freq, 0.4)
# Matriz de palavras frequentes
freq = TermDocumentMatrix(corpus_final)
freq
# Transformando em matrix do R
matriz = as.matrix(freq)
# Ordenando de acordo com a frequência
matriz = sort(rowSums(matriz), decreasing = TRUE)
# Data frame
matriz = data.frame(word = names(matriz), freq = matriz)
# Encontrando os 20 termos mais frequentes
findFreqTerms(freq, 20, Inf)
# Encontrando os termos mais frequentes, definindo como 20 o número mínimo de ocorrências
findFreqTerms(freq, 10, Inf)
# Carregando as bibliotecas
library(tm)
library(textreadr)
library(wordcloud)
library(RColorBrewer)
# Fontes disponíveis
getSources()
# Formatos
getReaders()
# Carregamento de um arquivo PDF salvo em disco
arquivo_cru = read_pdf("C:/Users/Usuario/Documents/Mestrado/FT095/Artigo/Recursos Humanos em saude_do processo intuitivo ao People Analytics.pdf", skip = 16)
# Filtrando apenas o conteúdo textual
arquivo_tratado = arquivo_cru[ ,3]
# Gerando um corpus
corpus_final = VCorpus(VectorSource(arquivo_tratado), readerControl = list(reader = readPlain, language = "eng"))
# Usando a função tm_map para executar as transformações
stopwords("portuguese")
# Removendo as stopwords e palavras sem sentido semântico pro objetivo final
# Retornar ao vetor "palavras" de forma iterativa, conforme ilustrado na Figura X, e ajustar os elementos
palavras = c("the", "ser", "silva", "nursing", "martinez", "martinez ", "francisco", "silva ", " silva", "Silva", "Martinez", "Francisco")
corpus_final = tm_map(corpus_final, removeWords, c(stopwords("portuguese"), palavras))
# Removendo o excesso de espaços em branco
corpus_final = tm_map(corpus_final, stripWhitespace)
# Removendo a pontuação
corpus_final = tm_map(corpus_final, removePunctuation)
# Removendo os números
corpus_final = tm_map(corpus_final, removeNumbers)
wordcloud(corpus_final, max.words = 20, random.order = T, colors = rainbow(8), rot.per = 0.5, use.r.layout = T)
# Matriz de palavras frequentes
freq = TermDocumentMatrix(corpus_final)
freq
# Transformando em matrix do R
matriz = as.matrix(freq)
# Ordenando de acordo com a frequência
matriz = sort(rowSums(matriz), decreasing = TRUE)
# Data frame
matriz = data.frame(word = names(matriz), freq = matriz)
head(matriz, n = 100)
# Encontrando os termos mais frequentes, definindo como 10 o número mínimo de ocorrências
findFreqTerms(freq, 10, Inf)
removeSparseTerms(freq, 0.4)
wordcloud(matriz$word, matriz$freq, max.words = 20, random.order = T, colors = rainbow(8), rot.per = 0.5, use.r.layout = T)
# Encontrando os termos mais frequentes, definindo como 10 o número mínimo de ocorrências
findFreqTerms(freq, 10, Inf)
removeSparseTerms(freq, 0.4)
# Data frame
matriz = data.frame(word = names(matriz), freq = matriz)
# Matriz de palavras frequentes
freq = TermDocumentMatrix(corpus_final)
freq
# Transformando em matrix do R
matriz = as.matrix(freq)
# Ordenando de acordo com a frequência
matriz = sort(rowSums(matriz), decreasing = TRUE)
head(matriz, n = 100)
# Carregando as bibliotecas
library(tm)
library(textreadr)
library(wordcloud)
library(RColorBrewer)
# Fontes disponíveis
getSources()
# Formatos
getReaders()
# Carregamento de um arquivo PDF salvo em disco
arquivo_cru = read_pdf("C:/Users/Usuario/Documents/Mestrado/FT095/Artigo/Recursos Humanos em saude_do processo intuitivo ao People Analytics.pdf", skip = 16)
# Filtrando apenas o conteúdo textual
arquivo_tratado = arquivo_cru[ ,3]
# Gerando um corpus
corpus_final = VCorpus(VectorSource(arquivo_tratado), readerControl = list(reader = readPlain, language = "eng"))
# Usando a função tm_map para executar as transformações
stopwords("portuguese")
# Removendo as stopwords e palavras sem sentido semântico pro objetivo final
# Retornar ao vetor "palavras" de forma iterativa, conforme ilustrado na Figura X, e ajustar os elementos
palavras = c("the", "ser", "silva", "nursing", "martinez", "martinez ", "francisco", "silva ", " silva", "Silva", "Martinez", "Francisco")
corpus_final = tm_map(corpus_final, removeWords, c(stopwords("portuguese"), palavras))
# Removendo o excesso de espaços em branco
corpus_final = tm_map(corpus_final, stripWhitespace)
# Removendo a pontuação
corpus_final = tm_map(corpus_final, removePunctuation)
# Removendo os números
corpus_final = tm_map(corpus_final, removeNumbers)
# Matriz de palavras frequentes
freq = TermDocumentMatrix(corpus_final)
freq
# Transformando em matrix do R
matriz = as.matrix(freq)
# Ordenando de acordo com a frequência
matriz = sort(rowSums(matriz), decreasing = TRUE)
# Data frame
matriz = data.frame(word = names(matriz), freq = matriz)
head(matriz, n = 100)
library(tm)
stopwords("portuguese")
lista = data.frame(lista)
lista = stopwords("portuguese")
lista = data.frame(lista)
lista
View(lista)
lista_dataframe = data.frame(lista)
lista = matrix(lista, 10, 20)
lista
lista = stopwords("portuguese")
lista
lista = as.matrix(lista)
lista
lista = as.matrix(lista, ncol = 10)
lista
lista = as.matrix(lista, ncol = 10, nrow = 20)
lista
lista = stopwords("portuguese")
lista
print(lista)
plot(lista)
teste = as.table(lista)
teste = as.data.frame.model.matrix(lista)
lista
teste = matrix(lista, ncol = 5, byrow = FALSE)
teste
teste = matrix(lista, ncol = 5, byrow = TRUE)
teste
teste = matrix(lista, ncol = 6, byrow = TRUE)
lista_dataframe
teste = matrix(lista, ncol = 5, byrow = TRUE)
teste
200/5
wordcloud(matriz$word, matriz$freq, max.words = 20, random.order = T, colors = rainbow(8), rot.per = 0.5, use.r.layout = T)
# Removendo as stopwords e palavras sem sentido semântico pro objetivo final
# Retornar ao vetor "palavras" de forma iterativa, conforme ilustrado na Figura X, e ajustar os elementos
palavras = c("the", "ser", "silva", "nursing", "martinez", "martinez ", "francisco", "silva ", " silva", "Silva", "Martinez", "Francisco", "grande")
corpus_final = tm_map(corpus_final, removeWords, c(stopwords("portuguese"), palavras))
# Removendo o excesso de espaços em branco
corpus_final = tm_map(corpus_final, stripWhitespace)
# Usando a função tm_map para executar as transformações
stopwords("portuguese")
# Removendo as stopwords e palavras sem sentido semântico pro objetivo final
# Retornar ao vetor "palavras" de forma iterativa, conforme ilustrado na Figura X, e ajustar os elementos
palavras = c("the", "ser", "silva", "nursing", "martinez", "martinez ", "francisco", "silva ", " silva", "Silva", "Martinez", "Francisco", "grande")
corpus_final = tm_map(corpus_final, removeWords, c(stopwords("portuguese"), palavras))
# Removendo o excesso de espaços em branco
corpus_final = tm_map(corpus_final, stripWhitespace)
# Removendo a pontuação
corpus_final = tm_map(corpus_final, removePunctuation)
# Removendo os números
corpus_final = tm_map(corpus_final, removeNumbers)
# Matriz de palavras frequentes
freq = TermDocumentMatrix(corpus_final)
freq
# Transformando em matrix do R
matriz = as.matrix(freq)
# Ordenando de acordo com a frequência
matriz = sort(rowSums(matriz), decreasing = TRUE)
# Data frame
matriz = data.frame(word = names(matriz), freq = matriz)
head(matriz, n = 100)
# Encontrando os termos mais frequentes, definindo como 10 o número mínimo de ocorrências
findFreqTerms(freq, 10, Inf)
removeSparseTerms(freq, 0.4)
# Removendo as stopwords e palavras sem sentido semântico pro objetivo final
# Retornar ao vetor "palavras" de forma iterativa, conforme ilustrado na Figura X, e ajustar os elementos
palavras = c("the", "ser", "silva", "nursing", "martinez", "martinez ", "francisco", "silva ", " silva", "Silva", "Martinez", "Francisco", "grande", "partir", "and")
# Usando a função tm_map para executar as transformações
stopwords("portuguese")
# Removendo as stopwords e palavras sem sentido semântico pro objetivo final
# Retornar ao vetor "palavras" de forma iterativa, conforme ilustrado na Figura X, e ajustar os elementos
palavras = c("the", "ser", "silva", "nursing", "martinez", "martinez ", "francisco", "silva ", " silva", "Silva", "Martinez", "Francisco", "grande", "partir", "and")
corpus_final = tm_map(corpus_final, removeWords, c(stopwords("portuguese"), palavras))
# Removendo o excesso de espaços em branco
corpus_final = tm_map(corpus_final, stripWhitespace)
# Removendo a pontuação
corpus_final = tm_map(corpus_final, removePunctuation)
# Removendo os números
corpus_final = tm_map(corpus_final, removeNumbers)
# Matriz de palavras frequentes
freq = TermDocumentMatrix(corpus_final)
freq
# Transformando em matrix do R
matriz = as.matrix(freq)
# Ordenando de acordo com a frequência
matriz = sort(rowSums(matriz), decreasing = TRUE)
# Data frame
matriz = data.frame(word = names(matriz), freq = matriz)
head(matriz, n = 100)
# Encontrando os termos mais frequentes, definindo como 10 o número mínimo de ocorrências
findFreqTerms(freq, 10, Inf)
removeSparseTerms(freq, 0.4)
wordcloud(matriz$word, matriz$freq, max.words = 20, random.order = T, colors = rainbow(8), rot.per = 0.5, use.r.layout = T)
wordcloud(matriz$word, matriz$freq, max.words = 20, random.order = T, colors = rainbow(8), rot.per = 0.5, use.r.layout = T)
# Usando a função tm_map para executar as transformações
stopwords("portuguese")
# Removendo as stopwords e palavras sem sentido semântico pro objetivo final
# Retornar ao vetor "palavras" de forma iterativa, conforme ilustrado na Figura X, e ajustar os elementos
palavras = c("the", "ser", "silva", "nursing", "martinez", "martinez ", "francisco", "silva ", " silva", "Silva", "Martinez", "Francisco", "grande", "partir", "and", "uso", "utilização")
corpus_final = tm_map(corpus_final, removeWords, c(stopwords("portuguese"), palavras))
# Removendo o excesso de espaços em branco
corpus_final = tm_map(corpus_final, stripWhitespace)
# Removendo a pontuação
corpus_final = tm_map(corpus_final, removePunctuation)
# Removendo os números
corpus_final = tm_map(corpus_final, removeNumbers)
# Matriz de palavras frequentes
freq = TermDocumentMatrix(corpus_final)
freq
# Transformando em matrix do R
matriz = as.matrix(freq)
# Ordenando de acordo com a frequência
matriz = sort(rowSums(matriz), decreasing = TRUE)
# Data frame
matriz = data.frame(word = names(matriz), freq = matriz)
head(matriz, n = 100)
# Encontrando os termos mais frequentes, definindo como 10 o número mínimo de ocorrências
findFreqTerms(freq, 10, Inf)
removeSparseTerms(freq, 0.4)
wordcloud(matriz$word, matriz$freq, max.words = 20, random.order = T, colors = rainbow(8), rot.per = 0.5, use.r.layout = T)
palavras
# Instalando as bibliotecas necessárias
install.packages("tm")
install.packages("textreadr")
install.packages("wordcloud")
install.packages("RColorBrewer")
# Carregando as bibliotecas
library(tm)
library(textreadr)
library(wordcloud)
library(RColorBrewer)
# Fontes disponíveis
getSources()
# Formatos
getReaders()
# Carregamento de um arquivo PDF salvo em disco
arquivo_cru = read_pdf("C:/Users/Usuario/Documents/Mestrado/FT095/Artigo/Recursos Humanos em saude_do processo intuitivo ao People Analytics.pdf", skip = 16)
# Filtrando apenas o conteúdo textual
arquivo_tratado = arquivo_cru[ ,3]
# Gerando um corpus
corpus_final = VCorpus(VectorSource(arquivo_tratado), readerControl = list(reader = readPlain, language = "eng"))
# Usando a função tm_map para executar as transformações
stopwords("portuguese")
# Removendo as stopwords e palavras sem sentido semântico pro objetivo final
# Retornar ao vetor "palavras" de forma iterativa, conforme ilustrado na Figura X, e ajustar os elementos
palavras = c("the", "ser", "silva", "nursing", "martinez", "martinez ", "francisco", "silva ", " silva", "Silva", "Martinez", "Francisco", "grande", "partir", "and", "uso", "utilização")
corpus_final = tm_map(corpus_final, removeWords, c(stopwords("portuguese"), palavras))
# Removendo o excesso de espaços em branco
corpus_final = tm_map(corpus_final, stripWhitespace)
# Removendo a pontuação
corpus_final = tm_map(corpus_final, removePunctuation)
# Removendo os números
corpus_final = tm_map(corpus_final, removeNumbers)
# Matriz de palavras frequentes
freq = TermDocumentMatrix(corpus_final)
freq
# Transformando em matrix do R
matriz = as.matrix(freq)
# Ordenando de acordo com a frequência
matriz = sort(rowSums(matriz), decreasing = TRUE)
# Data frame
matriz = data.frame(word = names(matriz), freq = matriz)
head(matriz, n = 100)
# Encontrando os termos mais frequentes, definindo como 10 o número mínimo de ocorrências
findFreqTerms(freq, 10, Inf)
removeSparseTerms(freq, 0.4)
write.csv(matriz, "C:\Users\Usuario\Documents\Mestrado\FT095\Artigo\matriz.csv")
write.csv(matriz, "C:/Users/Usuario/Documents/Mestrado/FT095/Artigo/matriz.csv")
library(textreadr)
# Instalando as bibliotecas necessárias
#install.packages("tm")
#install.packages("textreadr")
#install.packages("wordcloud")
#install.packages("RColorBrewer")
# Carregando as bibliotecas
library(tm)
library(textreadr)
library(wordcloud)
library(RColorBrewer)
# Fontes disponíveis
getSources()
# Formatos
getReaders()
# Carregamento de um arquivo PDF salvo em disco
arquivo_cru = read_pdf("C:/Users/Usuario/Documents/Mestrado/FT095/Artigo/Recursos Humanos em saude_do processo intuitivo ao People Analytics.pdf", skip = 16)
# Filtrando apenas o conteúdo textual
arquivo_tratado = arquivo_cru[ ,3]
# Gerando um corpus
corpus_final = VCorpus(VectorSource(arquivo_tratado), readerControl = list(reader = readPlain, language = "eng"))
# Usando a função tm_map para executar as transformações
stopwords("portuguese")
# Removendo as stopwords e palavras sem sentido semântico pro objetivo final
# Retornar ao vetor "palavras" de forma iterativa, conforme ilustrado na Figura X, e ajustar os elementos
palavras = c("the", "ser", "silva", "nursing", "martinez", "martinez ", "francisco", "silva ", " silva", "Silva", "Martinez", "Francisco", "grande", "partir", "and", "uso", "utilização")
corpus_final = tm_map(corpus_final, removeWords, c(stopwords("portuguese"), palavras))
# Removendo o excesso de espaços em branco
corpus_final = tm_map(corpus_final, stripWhitespace)
# Removendo a pontuação
corpus_final = tm_map(corpus_final, removePunctuation)
# Removendo os números
corpus_final = tm_map(corpus_final, removeNumbers)
wordcloud(corpus_final, max.words = 20, random.order = T, colors = rainbow(8), rot.per = 0.5, use.r.layout = T)
wordcloud(matriz$word, matriz$freq, max.words = 20, random.order = T, colors = rainbow(8), rot.per = 0.5, use.r.layout = T)
wordcloud(corpus_final, max.words = 22, random.order = T, colors = rainbow(8), rot.per = 0.5, use.r.layout = T)
wordcloud(corpus_final, max.words = 22, random.order = T, colors = rainbow(8), rot.per = 0.5, use.r.layout = T)
wordcloud(corpus_final, max.words = 22, random.order = T, colors = rainbow(8), rot.per = 0.5, use.r.layout = T)
# Encontrando os termos mais frequentes, definindo como 10 o número mínimo de ocorrências
findFreqTerms(freq, 10, Inf)
# Encontrando os termos mais frequentes, definindo como 10 o número mínimo de ocorrências
findFreqTerms(freq, 8, Inf)
wordcloud(matriz$word, matriz$freq, max.words = 22, random.order = T, colors = rainbow(8), rot.per = 0.5, use.r.layout = T)
install.packages("ggplot2")
install.packages("plotly")
install.packages("neuralnet")
install.packages("mltools")
install.packages("data.table")
install.packages("caret", dependencies = T)
library(neuralnet)
library(mltools)
library(data.table)
library(caret)
iris2 = scale(iris[, 1:4])
iris2 = as.data.frame(iris2)
# Adiciona a classe
iris2$species = iris$Species
iris2
set.seed(1234)
particao = createDataPartition(1:dim(iris2)[1], p = .7)
iristreino = iris2[particao$Resample1, ]
iristeste = iris2[- particao$Resample1, ]
dim(iristreino)
dim(iristeste)
# Juntamos os atributos com a classe para não perdê-los
iristreino = cbind(iristreino[, 1:4], one_hot(as.data.table(iristreino[, 5])))
iristreino
modelo = neuralnet(V1_setosa + V1_versicolor + V1_virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, iristreino, hidden = c(5, 4))
print(modelo)
plot(modelo)
teste = compute(modelo, iristeste[, 1:4])
teste$net.result
resultado = as.data.frame(teste$net.result)
resultado
names(resultado)[1] <- 'setosa'
names(resultado)[2] <- 'versicolor'
names(resultado)[3] <- 'virginica'
resultado
resultado$class = colnames(resultado[, 1:3])[max.col(resultado[, 1:3], ties.method = 'first')]
resultado
confusao = table(resultado$class, iristeste$species)
confusao
sum(diag(confusao)*100/sum(confusao))
setwd("~/Mestrado/Disciplinas/FT087 - Planejamento e Análise Experimental/Códigos")
# Importação do dataset
dataset <- read.csv(file = "C:\Users\Usuario\Documents\Estudos\Data Science e Programação\Spotify API\playlists_songs_clusters.csv")
# Importação do dataset
dataset <- read.csv(file = "c:/Users/Usuario/Documents/Estudos/Data Science e Programação/Spotify API\playlists_songs_clusters.csv")
# Importação do dataset
dataset <- read.csv(file = "c:/Users/Usuario/Documents/Estudos/Data Science e Programação/Spotify API/playlists_songs_clusters.csv")
dataset
head(dataset)
source('~/Mestrado/Disciplinas/FT087 - Planejamento e Análise Experimental/Códigos/Aula_01_Atividade_2.R', encoding = 'UTF-8', echo=TRUE)
typeof(dataset)
do.call(rbind.data.frame, dataset)
dataset
typeof(dataset)
# Importação do dataset
dataset_list <- read.csv(file = "c:/Users/Usuario/Documents/Estudos/Data Science e Programação/Spotify API/playlists_songs_clusters.csv")
dataset <- data.frame(matrix(unlist(dataset_list), nrow=length(dataset_list), byrow=TRUE))
typeof(dataset)
# Importação do dataset
dataset_list <- read.csv(file = "c:/Users/Usuario/Documents/Estudos/Data Science e Programação/Spotify API/playlists_songs_clusters.csv")
# Importação do dataset
dataset_list <- read.csv(file = "c:/Users/Usuario/Documents/Estudos/Data Science e Programação/Spotify API/playlists_songs_clusters.csv")
typeof(dataset_list)
dataset <- as.data.frame(dataset_list)
typeof(dataset)
View(dataset_list)
View(dataset)
source('~/Mestrado/Disciplinas/FT087 - Planejamento e Análise Experimental/Códigos/Aula_01_Atividade_2.R', encoding = 'UTF-8', echo=TRUE)
# Selecionando uma amostra aleatória de 50 observações
iris
# Selecionando uma amostra aleatória de 50 observações
amostra = dataset[sample(nrow(dataset), 50), ]
amostra
View(amostra)
# Medidas de Centralidade
colnames(amostra)
amostra
# Medidas de Centralidade
colunas <- colnames(amostra)
# Medidas de Centralidade
colunas <- c(colnames(amostra))
colunas
# Medidas de Centralidade
colunas <- colnames(amostra)
for (i in colunas) {
print(mean(amostra$i))
}
print(i)
for (i in colunas) {
print(i)
}
print(amostra$i)
colunas <- colnames(amostra)
for (i in colunas) {
print(amostra$i)
}
amostra
for (i in colunas) {
print(mean(amostra$i))
}
colunas <- colnames(amostra)
for (i in colunas) {
print(i)
}
colunas <- colnames(amostra)
for (i in colunas) {
print(amostra$i[1:2])
}
amostra
colunas <- colnames(amostra)
for (i in colunas) {
a <- amostra$i[1:2]
print(a)
}
for (element in colunas) {
a <- amostra$element[1:2]
print(a)
}
for (element in colunas) {
a <- amostra[element]
print(a)
}
